{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friday 2/19: \n",
    "#### Business Understanding & Preliminary EDA\n",
    "* Repo Creation\n",
    "* Data Importation\n",
    "* Database Creation\n",
    "* Created initial data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturday 2/20: \n",
    "#### Data Understanding & EDA\n",
    "* Added ```.gitignore``` file in exploratory directory to exclude ```KingDB.db``` file.\n",
    "* Created ```lookup()``` function.\n",
    "* Created 2019 data frame.\n",
    "* Created ```nz19``` data frame of 2019 documents with non-zero sale prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunday 2/21: \n",
    "#### Data Prep\n",
    "* Added a `function.py` module to contain the functions written while working through the project.\n",
    "* Created a `col_stripper()` function, and appended it, `fetch()`, and `lookup()` to the `function` module.\n",
    "* Did some data cleaning on the lookup dataframe `df_look`.\n",
    "* Made a `heat_df` data frame with the `'SalePrice'` target and `'HeatSystems'` predictor.\n",
    "* Converted  `'HeatSystems'` to a column called `'HeatNames'`with more descriptive values.\n",
    "* Perfomed one-hot encoding on `'HeatNames'` and created a `model_df`.\n",
    "* Created a correlation matrix and heatmap for `model_df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monday 2/22:\n",
    "#### Data Prep\n",
    "* Added `chunker()` and `dropper()` data cleaning functions.\n",
    "* Consolidated SQL notebook.\n",
    "* Started dropping features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuesday 2/23:\n",
    "#### Data Understanding, Project Environment & Chronicling\n",
    "* Got a better grip on the data.\n",
    "* Created `king_co` virtual environment for the project.\n",
    "* Started a project journal `Journal.ipynb` notebook.\n",
    "* Added a notebook `Notes.ipynb` for project notes and question.\n",
    "* Made an `ENV_Flow.md` file with environment creation instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday 2/23:\n",
    "#### Data Understanding & Data Prep\n",
    "* Dove deep into the data, figured it out and dropped corrupt rows.\n",
    "* Created a `print_uniques()` function.\n",
    "* Isolated 2019 data.\n",
    "* Got a better solution for naming one-hot columns\n",
    "* Made `PropType` lookup code data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thursday 2/24:\n",
    "#### Data Prep\n",
    "* Dug into the `ZipCode` feature.\n",
    "* Cleaned zip codes and added `ZipClean` feature.\n",
    "* Found that 18% of the rows have null zip code data.\n",
    "* Found that rows with null zip code have a mean sale price \\$117,000 higher.\n",
    "* Chunked out all of the feature columns for closer inspection tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friday 2/25\n",
    "#### Data Prep & Simple LR Model\n",
    "* After filtering for single family homes, 13% have null zips and the mean sale price of those homes is \\$164,717 higher.\n",
    "* Ran a t-test and found a statistically significant difference in the mean sale price from home with null zip codes when compared to homes with valid zip codes.\n",
    "* Since unpacking the relevant features that account for this difference is beyond the scope of this project, I decided to drop the null zip codes to keep the zip code feature fully quantified in the model.\n",
    "* Cleaned continuous features.\n",
    "* Made a correlation matrix for the continuous features.\n",
    "* Created a raw baseline model with `SalePrice` target and `SqFtTotLiving` predictor (r-sq.=.371).\n",
    "* The intercept was not statistically significant (p=.15, alpha=.05).\n",
    "* Checked LR assumptions (rainbow: pass, Jarque-Bera: fail, Breusch-Pagan: fail, Independence: pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturday 2/26\n",
    "#### LR Refinement, Data Cleaning & Feature Engineering\n",
    "* Created 4 more simple LR models with outliers removed and various log-transforms.\n",
    "* Model 3 (logged predictor, raw target with outliers removed) performed the best. R-squared=.268, with valid coefficients and CIs. (rainbow: pass, Jarque-Bera: fail, Breusch-Pagan: fail, Independence: pass).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunday 2/27\n",
    "#### LR Refinement & Feature Engineering\n",
    "* Created logger(), log_interpret(), log_target(), log_all() functions.\n",
    "* Created 4 more ols models. Model 3 is still the best combination of r and assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monday 2/28\n",
    "#### LR Refinement Data Processing\n",
    "* Made `HeatSystem` and partial `DistrictName` model.\n",
    "* Categorical data has proven problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
